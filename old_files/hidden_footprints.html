<!DOCTYPE html>
<html>
<body>

<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-176053367-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-176053367-1');
</script>

<title>Hidden Footprints</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
* {box-sizing: border-box}

/* Set height of body and the document to 100% */
body, html {
  margin: 0;
  font-family: Arial;
  background-color: #fffdf9;
}

/* Style tab links */
.tablink {
  background-color: #555;
  color: white;
  border: none;
  outline: none;
  cursor: pointer;
  padding: 14px 16px;
  font-size: 17px;
  width: 15%;
}

.tablink:hover {
  background-color: #777;
}

a {
      color: #2b8da5;
      text-decoration: none;
}

a:hover {
      color: #7fe1d9;
  }

 .topcorner{
   position:absolute;
   top:0%;
  }

  h2 {
      text-align: center;
  }

  .bib {
    font-size:14px;
    font-family:monospace;
    text-align:left;
  }
.video-responsive{
    overflow:hidden;
    padding-bottom:56.25%;
    position:relative;
    height:0;
}
.video-responsive iframe{
    left:0;
    top:0;
    width:100%;
    height:100%;
    position:absolute;
}

.mydiv{
    text-align:center;
    width:80%;
    margin:auto;
}

hr {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
}
</style>
</head>

<!--<div class="topcorner" style="left:12%">-->
<!--<a href="https://eccv2020.eu/"><img src="eccv.png" width="90"></a>-->
<!--</div>-->

<!--<div class="topcorner" style="right:5%">-->
<!--<a href="https://www.tech.cornell.edu/"><img src="cornelltech.png" height="90"></a>-->
<!--</div>-->


<div class="mydiv">

    <h1>Hidden Footprints:</h1>
    <h2>Learning Contextual Walkability from 3D Human Trails</h2>
    <table style="width:100%;margin:auto">
      <tr>
          <td style="width:25%;margin:auto">
          <span style="font-size:19px;"><a href="https://www.cs.cornell.edu/~jinsun/">Jin Sun</a></span>
        </td>

        <td style="width:25%;margin:auto">
          <span style="font-size:19px;"><a href="http://www.cs.cornell.edu/~hadarelor/">Hadar Averbuch-Elor</a></span>
        </td>

        <td style="width:25%;margin:auto">
          <span style="font-size:19px;"><a href="http://www.cs.cornell.edu/~qqw/">Qianqian Wang</a></span>
        </td>

        <td style="width:25%;margin:auto">
          <center>
          <span style="font-size:19px;"><a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a></span>
          </center>
        </td>
     </tr>
     </table>
    <br />
    <table style="width:100%;margin:auto">
      <tr>
        <td>
          <span style="font-size:20px">Cornell Tech</span>
        </td>
      </tr>
    </table>

      <center>
          <h3><i>The European Conference on Computer Vision (ECCV) 2020</i></h3>
      </center>
    <hr>

</div>

<div class="mydiv">
    <p style="text-align:left">
    <b>Abstract</b>. Predicting where people can walk in a scene is important for
    many tasks, including autonomous driving systems and human behavior
    analysis. Yet learning a computational model for this purpose is chal-
    lenging due to semantic ambiguity and a lack of labeled data: current
    datasets only tell you where people are, not where they could be. We tackle
    this problem by leveraging information from existing datasets, without
    additional labeling. We first augment the set of valid, labeled walkable
    regions by propagating person observations between images, utilizing
    3D information to create what we call hidden footprints. However, this
    augmented data is still sparse. We devise a training strategy designed for
    such sparse labels, combining a class-balanced classification loss with a
    contextual adversarial loss. Using this strategy, we demonstrate a model
    that learns to predict a walkability map from a single image. We evaluate our model on the Waymo and Cityscapes datasets, demonstrating
    superior performance compared to baselines and state-of-the-art models.
    </p>

      <table style="width:100%;margin:auto">
      <tr>
        <td style="width:33%">
            <span style="font-size:20px"><a href="http://arxiv.org/abs/2008.08701">ArXiv</a></span>
        </td>

        <td style="width:33%">
            <span style="font-size:20px"><a href="https://github.com/jinsungit/hiddenfootprints">Code</a></span>
        </td>
        <td style="width:33%">
            <span style="font-size:20px"><a href="hiddenfootprints/hidden-footprints-supp/index.html">More Results</a></span>
      </tr>
    </table>


</div>
<br />
<div class="mydiv">
    <div class="video-responsive">
<iframe width="560" height="315" src="https://www.youtube.com/embed/Yd4wX_vPOsw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<hr>
</div>

<div class="mydiv">
    <h2>Contextual Walkability</h2>
    <p><img src="hiddenfootprints/contextual walkability.png" width="100%"></p>
    <p style="text-align:left">(a) Imagine an urban scene like this one. Where could people walk in such
    a scene? The answer is not simply ‘crosswalk’ or ‘sidewalk’—scene context plays
    a significant role. For example, people can be standing on sidewalks and not on
    crosswalks while waiting for a green light (b, top). A road can be open to walk,
    or only partially walkable (e.g., when occupied by cows, as in (b, bottom)). (c)
    We propose to predict where people could walk (highlighted in green) by learning
    from observing human behavior in weakly annotated data.
    </p>
<hr>
</div>

<!--<hr>-->


<div class="mydiv">
    <h2>Hidden Footprints</h2>
    <p><img src="hiddenfootprints/footprints1.png" width="50%"><img src="hiddenfootprints/footprints2.png" width="50%"></p>
    <p style="text-align:left">
    We leverage sequences collected by recent self-driving datasets such as the <a href="https://waymo.com/open/">Waymo Open Dataset</a>, <b>without additional labeling</b>. 
    Our key observation is: a person’s 3D location in each frame is also a valid location in every other frame in the same sequence.
    </p>
    <p style="text-align:left">
    Given 3D labels, poses, and camera parameters, we propagate sparse ground-truth people locations to all frames: obtaining what we call Hidden Footprints.
    </p>

<hr>
</div>


<div class="mydiv">
    <h2>Results</h2>
    <p style="text-align:left">
    We also propose a training strategy to learn from hidden footprints and predict walkability maps. The model generalizes well to diverse scenes and outperforms alternative approaches including semantics-based ones.
    </p>
    <p style="text-align:center"><img src="hiddenfootprints/results.png" width="90%"></p>

    <p style="text-align:left">Below we show side-by-side video comparisons of input video sequences, propagated footprints, and our predicted heatmaps on the <b>validation set</b> of the Waymo dataset. Greener means higher score for walkability. Our model is able to predict reasonable walkable regions of diverse scenes, and under day/night conditions.</p>
     <p style="text-align:left">Please select different sequences below to play and click 'Play/Pause' for playback controls.</p>
    <div style="height:700px" class="video-responsive">
    <iframe src="hiddenfootprints/hidden-footprints-supp/videos/index.html" width="100%" height="100%" frameBorder="0"></iframe>
    </div>
<hr>
</div>

<div class="mydiv">
    <h2>Bibtex</h2>
    <div class="bib">
    @InProceedings{hiddenfootprints2020eccv,
    <br />
        title={Hidden Footprints: Learning Contextual Walkability from 3D Human Trails},
    <br />
        author={Jin Sun and Hadar Averbuch-Elor and Qianqian Wang and Noah Snavely},
    <br />
        booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
    <br />
	month={August},
    <br />
        year={2020}
    }
    </div>
<br />
<hr>
</div>


<div class="mydiv">
    <h2>Acknowledgements</h2>
    <p style="text-align:left">This research was supported in part by the generosity of
    Eric and Wendy Schmidt by recommendation of the Schmidt Futures program.
    </p>
</div>



</body>
</html>
